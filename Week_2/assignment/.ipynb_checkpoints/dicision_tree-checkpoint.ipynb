{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree exercise\n",
    "You should use decision tree to classify. \n",
    "\n",
    "Design your DecisionTree. Do binary classification or multiclass classification (selected by yourself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import log\n",
    "import operater\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle in-class Competetion\n",
    "请先前往 Kaggle 下载本次比赛的数据集\n",
    "\n",
    "比赛页面：https://inclass.kaggle.com/c/hdu-cama/leaderboard\n",
    "\n",
    "本次比赛可使用的 Package: Pandas, Numpy 以及系统内置库如 math 等\n",
    "\n",
    "完成下面代码后，使用 predict 函数对 test.csv 中的数据做出预测并将结果保存至一个 .csv 文件，然后 submit 至 Kaggle，可参考示例文件 sample.csv\n",
    "\n",
    "__请务必仔细阅读 Kaggle 页面的各项信息__\n",
    "\n",
    "__请务必仔细阅读 Kaggle 页面的各项信息__\n",
    "\n",
    "__请务必仔细阅读 Kaggle 页面的各项信息__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 请务必仔细阅读文件 “ID3 Algorithm for Decision Trees.pdf”\n",
    "## 请务必仔细阅读文件 “ID3 Algorithm for Decision Trees.pdf”\n",
    "## 请务必仔细阅读文件 “ID3 Algorithm for Decision Trees.pdf”\n",
    "### Calculate Shannon Entropy\n",
    "\n",
    "熵是对不确定性的测量，熵越高，代表信息量越高，这里你需要使用熵来选择作为节点的特征。（选择能够最小化两边熵的特征）\n",
    "\n",
    "$$Entropy(S) = - P_+ \\log_2{P_+} - P_- \\log_2{P_-}$$\n",
    "\n",
    "### Calculate Information Gain\n",
    "$$Gain(S, A) = Entropy(S) - \\sum_{v\\in Values(A)}{\\frac{|S_v|}{|S|}Entropy(S_v)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data from train.csv and y_train.csv\n",
    "train_feature = pd.read_csv('train.csv')\n",
    "train_label = pd.read_csv('y_train.csv')\n",
    "test_feature = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateShannonEntropy(features, labels):\n",
    "    # Todo 1: calculate the entropy given a dataset\n",
    "   \n",
    "    numEntries = len(features)\n",
    "    labelCounts = {}\n",
    "\n",
    "    for labelVector in labels:\n",
    "    \tcurrentLabel = labelVector[-1]\n",
    "    \tif currentLabel not in labelCounts.keys():\n",
    "    \t\tlabelCounts[currentLabel] = 0\n",
    "    \tlabelCounts[currentLabel] += 1\n",
    "\n",
    "    total_entropy = 0.0\n",
    "    for key in labelCounts:\n",
    "    \tpropotion_k = float(labelCounts[key]) / numEntries\n",
    "    \ttotal_entropy += -(propotion_k * log(propotion_k, 2))\n",
    "\n",
    "    return total_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_best_feature_to_split(features, labels):\n",
    "    # Todo 2: return the best feature based on the maximum number of information gain\n",
    "    numFeatures = len(features[0]) - 1\n",
    "    baseEntropy = calculateShannonEntropy(features, labels)\n",
    "    bestInfoGain = 0\n",
    "    best_feature = 0\n",
    "\n",
    "    for i in range(numFeatures):\n",
    "    \tfeatureList = [number[i] for number in features] # enum for one attribute\n",
    "    \tuniqualValues = set(featureList) # no-relace attribute\n",
    "    \tnewEntropy = 0\n",
    "\n",
    "    \tfor value in uniqualValues:\n",
    "    \t\tsub_dataset = split_dataset(features, labels, current_feature, value)\n",
    "    \t\tproportion_k = len(sub_dataset) / float(len(features))\n",
    "    \t\tnewEntropy += proportion_k * calculateShannonEntropy(sub_dataset) # sum(ShannonEntropy)\n",
    "    \tinfoGain = baseEntropy - newEntropy # infoGain\n",
    "\n",
    "    \t# bestInfoGain\n",
    "    \tif (infoGain > bestInfoGain):\n",
    "    \t\tbestInfoGain = infoGain\n",
    "    \t\tbest_feature = i\n",
    "\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(features, labels, current_feature, value):\n",
    "    # Todo 3: Split the dataset via current selected feature and it's value\n",
    "    # For example, when current_feature is TLS(top-left-square), and the value is 'o', \n",
    "    # the task is that return the subdataset in which all \"TLS\" is equal to 'o'\n",
    "    sub_dataset = []\n",
    "\n",
    "    for featureVector in features:\n",
    "    \tif current_feature == value:\n",
    "    \t\treduceFeatureVector = featureVector[ :axis]\n",
    "    \t\treduceFeatureVector.extend(featureVector[axis+1: ])  \n",
    "    \t\tsub_dataset.append(reduceFeatureVector)\n",
    "\n",
    "    return sub_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_decision_tree(features, labels):\n",
    "    # Todo 4: Create a decision tree by recursion\n",
    "    #\n",
    "    # Tips: Set appropriate boundary conditions; \n",
    "    #       think about the values one by one; \n",
    "    #       Use the three functions defined before.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get test_dataset from test.csv\n",
    "\n",
    "def predict(decision_tree, test_dataset):\n",
    "    # Todo 5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化（可选）\n",
    "在上面这些步骤完成后，你可以优化 create_decision_tree 函数以防止过拟合\n",
    "\n",
    "- 对决策树进行剪枝\n",
    "- 也推荐两个更简单又十分有效的办法\n",
    "    - 设置树的最大深度 max_depth\n",
    "    - 设置每个叶节点的最小 samples 数\n",
    "    - 这里可以参考 [decision tree in scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) 中的参数设置以及其原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估\n",
    "\n",
    "下面的数据可在你的 predict 文件提交至 Kaggle 后获得。\n",
    "\n",
    "- Kaggle 昵称：\n",
    "- 模型目前 Public Leaderboard 得分：\n",
    "- 排名："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反思\n",
    "请对你的模型进行一定的分析，说出你模型的不足之处，或者可以提高的地方。\n",
    "\n",
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
