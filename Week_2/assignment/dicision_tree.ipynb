{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree exercise\n",
    "You should use decision tree to classify. \n",
    "\n",
    "Design your DecisionTree. Do binary classification or multiclass classification (selected by yourself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import tree\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle in-class Competetion\n",
    "请先前往 Kaggle 下载本次比赛的数据集\n",
    "\n",
    "比赛页面：https://inclass.kaggle.com/c/hdu-cama/leaderboard\n",
    "\n",
    "本次比赛可使用的 Package: Pandas, Numpy 以及系统内置库如 math 等\n",
    "\n",
    "完成下面代码后，使用 predict 函数对 test.csv 中的数据做出预测并将结果保存至一个 .csv 文件，然后 submit 至 Kaggle，可参考示例文件 sample.csv\n",
    "\n",
    "__请务必仔细阅读 Kaggle 页面的各项信息__\n",
    "\n",
    "__请务必仔细阅读 Kaggle 页面的各项信息__\n",
    "\n",
    "__请务必仔细阅读 Kaggle 页面的各项信息__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 请务必仔细阅读文件 “ID3 Algorithm for Decision Trees.pdf”\n",
    "## 请务必仔细阅读文件 “ID3 Algorithm for Decision Trees.pdf”\n",
    "## 请务必仔细阅读文件 “ID3 Algorithm for Decision Trees.pdf”\n",
    "### Calculate Shannon Entropy\n",
    "\n",
    "熵是对不确定性的测量，熵越高，代表信息量越高，这里你需要使用熵来选择作为节点的特征。（选择能够最小化两边熵的特征）\n",
    "\n",
    "$$Entropy(S) = - P_+ \\log_2{P_+} - P_- \\log_2{P_-}$$\n",
    "\n",
    "### Calculate Information Gain\n",
    "$$Gain(S, A) = Entropy(S) - \\sum_{v\\in Values(A)}{\\frac{|S_v|}{|S|}Entropy(S_v)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from train.csv and y_train.csv\n",
    "train_feature = pd.read_csv('train.csv')\n",
    "train_label = pd.read_csv('y_train.csv')\n",
    "\n",
    "train_dataSet = pd.merge(train_feature, tarin_label, on = 'ID')\n",
    "\n",
    "featureNames = train_dataSet.columns.tolist()[1:10]\n",
    "\n",
    "dataSet_inArray = train_dataSet.values\n",
    "dataSet_inArray_noSerial = dataSet_inArray[:,1:]\n",
    "dataSet_totalSplit = dataSet_inArray_noSerial.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9278532379384186"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculateShannonEntropy(dataSet):\n",
    "    # Todo 1: calculate the entropy given a dataset\n",
    "   \n",
    "    numEntries = len(dataSet) # There are n rows inside\n",
    "    labelCounts = {} # Create dictionary for classification\n",
    "\n",
    "    for featureVector in dataSet:\n",
    "        \n",
    "    \tcurrentLabel = featureVector[-1] # Get the last-row data\n",
    "    \tif currentLabel not in labelCounts.keys():\n",
    "    \t\tlabelCounts[currentLabel] = 0\n",
    "    \tlabelCounts[currentLabel] += 1\n",
    "\n",
    "    total_entropy = 0.0\n",
    "    for key in labelCounts:\n",
    "    \tproportion_k = float(labelCounts[key]) / numEntries\n",
    "    \ttotal_entropy -= (proportion_k * log(proportion_k, 2))\n",
    "\n",
    "    return total_entropy\n",
    "\n",
    "calculateShannonEntropy(dataSet_totalSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choose_best_feature_to_split(dataSet):\n",
    "    # Todo 2: return the best feature based on the maximum number of information gain\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    baseEntropy = calculateShannonEntropy(dataSet)\n",
    "    bestInfoGain = 0\n",
    "    best_feature = 0\n",
    "\n",
    "    for i in range(numFeatures):\n",
    "    \tfeatureList = [number[i] for number in dataSet] # enum for one attribute\n",
    "    \tuniqualValues = set(featureList) # no-relace attribute\n",
    "    \tnewEntropy = 0\n",
    "\n",
    "    \tfor value in uniqualValues:\n",
    "    \t\tsub_dataset = split_dataset(dataSet, i, value)\n",
    "    \t\tproportion_k = len(sub_dataset) / float(len(dataSet))\n",
    "    \t\tnewEntropy += proportion_k * calculateShannonEntropy(sub_dataset) # sum(ShannonEntropy)\n",
    "    \tinfoGain = baseEntropy - newEntropy # infoGain\n",
    "\n",
    "    \t# bestInfoGain\n",
    "    \tif (infoGain > bestInfoGain):\n",
    "    \t\tbestInfoGain = infoGain\n",
    "    \t\tbest_feature = i\n",
    "\n",
    "    return best_feature\n",
    "choose_best_feature_to_split(dataSet_totalSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataSet, axis, value):\n",
    "    # Todo 3: Split the dataset via current selected feature and it's value\n",
    "    # For example, when current_feature is TLS(top-left-square), and the value is 'o', \n",
    "    # the task is that return the subdataset in which all \"TLS\" is equal to 'o'\n",
    "    sub_dataset = []\n",
    "\n",
    "    for featureVector in dataSet:\n",
    "    \tif featureVector[axis] == value:\n",
    "    \t\treduceFeatureVector = featureVector[ :axis]\n",
    "    \t\treduceFeatureVector.extend(featureVector[axis+1: ])  \n",
    "    \t\tsub_dataset.append(reduceFeatureVector)\n",
    "\n",
    "    return sub_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "def majorityCnt(classList):\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():classCount[vote] = 0\n",
    "        classCount[vote]+=1\n",
    "    sortedClassCount=sorted(classCount.items(),key = operator.itemgetter(1),reverse = True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "def create_decision_tree(dataSet, featureNames):\n",
    "    # Todo 4: Create a decision tree by recursion\n",
    "    #\n",
    "    # Tips: Set appropriate boundary conditions; \n",
    "    #       think about the values one by one; \n",
    "    #       Use the three functions defined before.\n",
    "    \n",
    "    classList = [example[0] for example in dataSet]\n",
    "    #类别相同，停止划分\n",
    "    if classList.count(classList[-1]) == len(classList):\n",
    "        return classList[0]\n",
    "    #长度为1，返回出现次数最多的类别\n",
    "    if len(classList[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "\n",
    "    best_feature = choose_best_feature_to_split(dataSet) #返回分类的特征序号\n",
    "    bestFeatureName = featureNames[best_feature] #该特征的label\n",
    "    decision_tree = {bestFeatureName: { } }\n",
    "    del(featureNames[best_feature]) #从labels的list中删除该label\n",
    "    \n",
    "    featureValues = [example[best_feature] for example in dataSet]\n",
    "    uniqualValues = set(featureValues)\n",
    "    for value in uniqualValues:\n",
    "    \tsubFeatureNames = featureNames[ : ] #子集合\n",
    "\n",
    "    \t#构建数据的子集合，并进行递归\n",
    "    \tdecision_tree[bestFeatureName][value] = create_decision_tree(split_dataset(dataSet, best_feature, value), subFeatureNames)\n",
    "    \n",
    "    return decision_tree\n",
    "\n",
    "myDecisionTree = create_decision_tree(dataSet_totalSplit, featureNames)\n",
    "print(myDecisionTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test_dataset from test.csv\n",
    "test_dataset = pd.read_csv('test.csv')\n",
    "\n",
    "# Todo 5\n",
    "\n",
    "def classify(inputTree, featureNames, testVector):\n",
    "    \n",
    "    firstStr = inputTree.keys()[0] #获取树的第一个特征属性\n",
    "    secondDict = inputTree[firstStr] #树的分支，子集合Dict\n",
    "    featureIndex = featureNames.index(firstStr) #获取决策树第一层在featLables中的位置\n",
    "    for key in secondDict.keys():\n",
    "        if testVector[featureIndex] == key:\n",
    "            if type(secondDict[key]).__name__ == 'dict':\n",
    "                classLabel = classify(secondDict[key], featureNames, testVector)\n",
    "            else:\n",
    "            \tclassLabel = secondDict[key]\n",
    "    \n",
    "    return classLabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-256-2628b63c0aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassLabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDecisionTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureNames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mclassLabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#predict(myDecisionTree, featureNames, test_dataset):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-255-415e04dd3bed>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(inputTree, featureNames, testVector)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureNames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestVector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfirstStr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#获取树的第一个特征属性\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0msecondDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputTree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirstStr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#树的分支，子集合Dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfeatureIndex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureNames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirstStr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#获取决策树第一层在featLables中的位置\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "classLabel = classify(myDecisionTree, featureNames, ['o', 'x', 'o', 'b', 'o', 'x', 'o', 'x', 'x'] )\n",
    "print classLabel\n",
    "\n",
    "#predict(myDecisionTree, featureNames, test_dataset):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化（可选）\n",
    "在上面这些步骤完成后，你可以优化 create_decision_tree 函数以防止过拟合\n",
    "\n",
    "- 对决策树进行剪枝\n",
    "- 也推荐两个更简单又十分有效的办法\n",
    "    - 设置树的最大深度 max_depth\n",
    "    - 设置每个叶节点的最小 samples 数\n",
    "    - 这里可以参考 [decision tree in scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) 中的参数设置以及其原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估\n",
    "\n",
    "下面的数据可在你的 predict 文件提交至 Kaggle 后获得。\n",
    "\n",
    "- Kaggle 昵称：\n",
    "- 模型目前 Public Leaderboard 得分：\n",
    "- 排名："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反思\n",
    "请对你的模型进行一定的分析，说出你模型的不足之处，或者可以提高的地方。\n",
    "\n",
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
