{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree exercise\n",
    "You should use decision tree to classify. \n",
    "\n",
    "Design your DecisionTree. Do binary classification or multiclass classification (selected by yourself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle in-class Competetion\n",
    "请先前往 Kaggle 下载本次比赛的数据集\n",
    "\n",
    "比赛页面：https://inclass.kaggle.com/c/hdu-cama/leaderboard\n",
    "\n",
    "本次比赛可使用的 Package: Pandas, Numpy 以及系统内置库如 math 等\n",
    "\n",
    "完成下面代码后，使用 predict 函数对 test.csv 中的数据做出预测并将结果保存至一个 .csv 文件，然后 submit 至 Kaggle，可参考示例文件 sample.csv\n",
    "\n",
    "__请务必仔细阅读 Kaggle 页面的各项信息__\n",
    "\n",
    "__请务必仔细阅读 Kaggle 页面的各项信息__\n",
    "\n",
    "__请务必仔细阅读 Kaggle 页面的各项信息__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 请务必仔细阅读文件 “ID3 Algorithm for Decision Trees.pdf”\n",
    "## 请务必仔细阅读文件 “ID3 Algorithm for Decision Trees.pdf”\n",
    "## 请务必仔细阅读文件 “ID3 Algorithm for Decision Trees.pdf”\n",
    "### Calculate Shannon Entropy\n",
    "\n",
    "熵是对不确定性的测量，熵越高，代表信息量越高，这里你需要使用熵来选择作为节点的特征。（选择能够最小化两边熵的特征）\n",
    "\n",
    "$$Entropy(S) = - P_+ \\log_2{P_+} - P_- \\log_2{P_-}$$\n",
    "\n",
    "### Calculate Information Gain\n",
    "$$Gain(S, A) = Entropy(S) - \\sum_{v\\in Values(A)}{\\frac{|S_v|}{|S|}Entropy(S_v)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data from train.csv and y_train.csv\n",
    "train_feature = pd.read_csv('train.csv')\n",
    "train_label = pd.read_csv('y_train.csv')\n",
    "\n",
    "train_dataSet = pd.merge(train_feature, train_label, on = 'ID')\n",
    "\n",
    "featureNames = train_dataSet.columns.tolist()[1:10]\n",
    "featureNames_copy = train_dataSet.columns.tolist()[1:10]\n",
    "\n",
    "dataSet_inArray = train_dataSet.values\n",
    "dataSet_inArray_noSerial = dataSet_inArray[:,1:]\n",
    "dataSet_totalSplit = dataSet_inArray_noSerial.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateShannonEntropy(dataSet):\n",
    "    # Todo 1: calculate the entropy given a dataset\n",
    "   \n",
    "    numEntries = len(dataSet) # There are n rows inside\n",
    "    labelCounts = {} # Create dictionary for classification\n",
    "\n",
    "    for featureVector in dataSet:\n",
    "        \n",
    "    \tcurrentLabel = featureVector[-1] # Get the last-row data\n",
    "    \tif currentLabel not in labelCounts.keys():\n",
    "    \t\tlabelCounts[currentLabel] = 0\n",
    "    \tlabelCounts[currentLabel] += 1\n",
    "\n",
    "    total_entropy = 0.0\n",
    "    for key in labelCounts:\n",
    "    \tproportion_k = float(labelCounts[key]) / numEntries\n",
    "    \ttotal_entropy -= (proportion_k * math.log(proportion_k, 2))\n",
    "\n",
    "    return total_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9278532379384186"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO 1 Test:\n",
    "calculateShannonEntropy(dataSet_totalSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_best_feature_to_split(dataSet):\n",
    "    # Todo 2: return the best feature based on the maximum number of information gain\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    baseEntropy = calculateShannonEntropy(dataSet)\n",
    "    bestInfoGain = 0\n",
    "    best_feature = -1\n",
    "\n",
    "    for i in range(numFeatures):\n",
    "    \tfeatureList = [number[i] for number in dataSet] # enum for one attribute\n",
    "    \tuniqualValues = set(featureList) # no-relace attribute\n",
    "    \tnewEntropy = 0\n",
    "\n",
    "    \tfor value in uniqualValues:\n",
    "    \t\tsub_dataset = split_dataset(dataSet, i, value)\n",
    "    \t\tproportion_k = len(sub_dataset) / float(len(dataSet))\n",
    "    \t\tnewEntropy += proportion_k * calculateShannonEntropy(sub_dataset) # sum(ShannonEntropy)\n",
    "    \tinfoGain = baseEntropy - newEntropy # infoGain\n",
    "\n",
    "    \t# bestInfoGain\n",
    "    \tif (infoGain > bestInfoGain):\n",
    "    \t\tbestInfoGain = infoGain\n",
    "    \t\tbest_feature = i\n",
    "\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO 2 Test:\n",
    "choose_best_feature_to_split(dataSet_totalSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataSet, axis, value):\n",
    "    # Todo 3: Split the dataset via current selected feature and it's value\n",
    "    # For example, when current_feature is TLS(top-left-square), and the value is 'o', \n",
    "    # the task is that return the subdataset in which all \"TLS\" is equal to 'o'\n",
    "    sub_dataset = []\n",
    "\n",
    "    for featureVector in dataSet:\n",
    "    \tif featureVector[axis] == value:\n",
    "    \t\treduceFeatureVector = featureVector[ :axis]\n",
    "    \t\treduceFeatureVector.extend(featureVector[axis+1: ])  \n",
    "    \t\tsub_dataset.append(reduceFeatureVector)\n",
    "\n",
    "    return sub_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "def majorityCnt(classList):\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():classCount[vote] = 0\n",
    "        classCount[vote]+=1\n",
    "    sortedClassCount=sorted(classCount.items(),key = operator.itemgetter(1),reverse = True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "def create_decision_tree(dataSet, featureNames):\n",
    "    # Todo 4: Create a decision tree by recursion\n",
    "    #\n",
    "    # Tips: Set appropriate boundary conditions; \n",
    "    #       think about the values one by one; \n",
    "    #       Use the three functions defined before.\n",
    "    \n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    #类别相同，停止划分\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    #长度为1，返回出现次数最多的类别\n",
    "    if len(classList[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "\n",
    "    best_feature = choose_best_feature_to_split(dataSet) #返回分类的特征序号\n",
    "    bestFeatureName = featureNames[best_feature] #该特征的label\n",
    "    decision_tree = {bestFeatureName: { } }\n",
    "    del(featureNames[best_feature]) #从labels的list中删除该label\n",
    "    \n",
    "    featureValues = [example[best_feature] for example in dataSet]\n",
    "    uniqualValues = set(featureValues)\n",
    "    for value in uniqualValues:\n",
    "    \tsubFeatureNames = featureNames[ : ] #子集合\n",
    "\n",
    "    \t#构建数据的子集合，并进行递归\n",
    "    \tdecision_tree[bestFeatureName][value] = create_decision_tree(split_dataset(dataSet, best_feature, value), subFeatureNames)\n",
    "    \n",
    "    return decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MMS': {'b': {'BLS': {'b': {'TRS': {'o': 'negative', 'x': 'positive'}},\n",
       "    'o': {'TRS': {'b': 'negative',\n",
       "      'o': 'negative',\n",
       "      'x': {'TLS': {'b': {'MLS': {'o': 'positive', 'x': 'negative'}},\n",
       "        'o': {'TMS': {'b': 'negative', 'o': 'positive', 'x': 'negative'}},\n",
       "        'x': {'TMS': {'o': 'negative', 'x': 'positive'}}}}}},\n",
       "    'x': {'TRS': {'b': 'positive',\n",
       "      'o': {'TMS': {'b': 'positive',\n",
       "        'o': {'TLS': {'b': 'positive', 'o': 'negative', 'x': 'positive'}},\n",
       "        'x': {'BRS': {'b': 'positive',\n",
       "          'o': {'MRS': {'b': 'positive', 'o': 'negative'}},\n",
       "          'x': 'positive'}}}},\n",
       "      'x': 'positive'}}}},\n",
       "  'o': {'BRS': {'b': {'TLS': {'b': 'negative',\n",
       "      'o': 'negative',\n",
       "      'x': {'MLS': {'b': {'TRS': {'b': 'negative',\n",
       "          'o': 'negative',\n",
       "          'x': {'TMS': {'o': 'negative', 'x': 'positive'}}}},\n",
       "        'o': {'MRS': {'b': 'positive',\n",
       "          'o': 'negative',\n",
       "          'x': {'BLS': {'b': 'positive',\n",
       "            'o': {'TRS': {'o': 'negative', 'x': 'positive'}},\n",
       "            'x': 'negative'}}}},\n",
       "        'x': {'BLS': {'b': {'TMS': {'o': 'negative', 'x': 'positive'}},\n",
       "          'x': 'positive'}}}}}},\n",
       "    'o': {'TLS': {'b': 'negative',\n",
       "      'o': 'negative',\n",
       "      'x': {'BMS': {'b': 'positive',\n",
       "        'o': {'BLS': {'b': {'TMS': {'o': 'negative', 'x': 'positive'}},\n",
       "          'o': 'negative',\n",
       "          'x': 'positive'}},\n",
       "        'x': {'TMS': {'b': 'negative',\n",
       "          'o': {'TRS': {'o': 'positive',\n",
       "            'x': {'MRS': {'o': 'positive', 'x': 'negative'}}}},\n",
       "          'x': {'TRS': {'o': 'negative', 'x': 'positive'}}}}}}}},\n",
       "    'x': {'TRS': {'b': {'BMS': {'b': {'TMS': {'b': 'negative',\n",
       "          'o': 'positive',\n",
       "          'x': 'negative'}},\n",
       "        'o': {'TMS': {'b': 'positive', 'o': 'negative'}},\n",
       "        'x': {'TMS': {'b': 'positive', 'o': 'positive', 'x': 'negative'}}}},\n",
       "      'o': {'BLS': {'b': 'negative',\n",
       "        'o': 'negative',\n",
       "        'x': {'BMS': {'b': {'TMS': {'b': 'positive',\n",
       "            'o': {'TLS': {'o': 'negative', 'x': 'positive'}},\n",
       "            'x': 'negative'}},\n",
       "          'o': 'negative',\n",
       "          'x': 'positive'}}}},\n",
       "      'x': {'MRS': {'b': {'BMS': {'o': {'TMS': {'o': 'negative',\n",
       "            'x': 'positive'}},\n",
       "          'x': 'positive'}},\n",
       "        'o': {'MLS': {'b': 'positive',\n",
       "          'o': 'negative',\n",
       "          'x': {'TMS': {'o': 'negative',\n",
       "            'x': {'TLS': {'o': 'negative', 'x': 'positive'}}}}}},\n",
       "        'x': 'positive'}}}}}},\n",
       "  'x': {'BLS': {'b': {'MLS': {'b': {'MRS': {'b': 'positive',\n",
       "        'o': {'TRS': {'b': 'positive',\n",
       "          'o': {'BRS': {'o': 'negative', 'x': 'positive'}},\n",
       "          'x': 'positive'}},\n",
       "        'x': {'TMS': {'o': 'negative', 'x': 'positive'}}}},\n",
       "      'o': 'positive',\n",
       "      'x': {'MRS': {'b': 'positive',\n",
       "        'o': {'TLS': {'b': 'negative',\n",
       "          'o': {'TMS': {'o': 'negative', 'x': 'positive'}},\n",
       "          'x': 'positive'}},\n",
       "        'x': 'positive'}}}},\n",
       "    'o': {'TLS': {'b': {'MRS': {'b': {'MLS': {'b': 'negative',\n",
       "          'o': 'positive',\n",
       "          'x': 'negative'}},\n",
       "        'o': 'positive',\n",
       "        'x': 'positive'}},\n",
       "      'o': {'MLS': {'b': 'positive',\n",
       "        'o': 'negative',\n",
       "        'x': {'MRS': {'b': {'TRS': {'b': 'positive',\n",
       "            'o': {'TMS': {'o': 'negative', 'x': 'positive'}},\n",
       "            'x': 'negative'}},\n",
       "          'o': {'BMS': {'o': 'negative', 'x': 'positive'}},\n",
       "          'x': 'positive'}}}},\n",
       "      'x': {'BRS': {'b': 'positive',\n",
       "        'o': {'BMS': {'b': {'MRS': {'b': 'positive',\n",
       "            'o': 'negative',\n",
       "            'x': 'positive'}},\n",
       "          'o': 'negative',\n",
       "          'x': 'positive'}},\n",
       "        'x': 'positive'}}}},\n",
       "    'x': {'TRS': {'b': 'positive',\n",
       "      'o': {'BRS': {'b': {'TLS': {'b': 'positive',\n",
       "          'o': {'TMS': {'b': 'positive', 'o': 'negative', 'x': 'positive'}},\n",
       "          'x': 'positive'}},\n",
       "        'o': {'MRS': {'b': {'TLS': {'b': 'positive',\n",
       "            'o': 'negative',\n",
       "            'x': 'positive'}},\n",
       "          'o': 'negative',\n",
       "          'x': {'TLS': {'o': 'positive',\n",
       "            'x': {'TMS': {'o': 'positive', 'x': 'negative'}}}}}},\n",
       "        'x': {'TLS': {'b': 'positive',\n",
       "          'o': {'TMS': {'b': 'positive', 'o': 'negative', 'x': 'negative'}},\n",
       "          'x': 'positive'}}}},\n",
       "      'x': 'positive'}}}}}}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO 3&4 Test:\n",
    "myDecisionTree = create_decision_tree(dataSet_totalSplit, featureNames)\n",
    "myDecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Todo 5\n",
    "# Func: classify\n",
    "\n",
    "def classify(inputTree, featureNames, testVector):\n",
    "    classLabel = []\n",
    "    firstStr = inputTree.keys()[0] #获取树的第一个特征属性\n",
    "    secondDict = inputTree[firstStr] #树的分支，子集合Dict\n",
    "    featureIndex = featureNames.index(firstStr) #获取决策树第一层在featLables中的位置\n",
    "    for key in secondDict.keys():\n",
    "        if testVector[featureIndex] == key:\n",
    "            if type(secondDict[key]).__name__ == 'dict':\n",
    "                classLabel = classify(secondDict[key], featureNames, testVector)\n",
    "            else:\n",
    "            \tclassLabel = secondDict[key]\n",
    "    \n",
    "    return classLabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO 5 Test\n",
    "# Func: classify Test:\n",
    "classLabel = classify(myDecisionTree, featureNames_copy, ['o', 'x', 'o', 'b', 'o', 'x', 'o', 'x', 'x'] )\n",
    "classLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get test_dataset from test.csv\n",
    "test_feature = pd.read_csv('test.csv')\n",
    "\n",
    "#test_dataSet = test_feature.columns.tolist()[1:10]\n",
    "\n",
    "#tset_dataSet_inArray = test_dataset.values\n",
    "#test_dataSet_inArray_noSerial = tset_dataSet_inArray[:,1:]\n",
    "#tset_dataSet_totalSplit = dataSet_inArray_noSerial.tolist()\n",
    "test_dataSet_totalSplit = test_feature.values[:,1:]\n",
    "\n",
    "\n",
    "# Todo 5\n",
    "\n",
    "def predict(myDecisionTree, featureNames, test_dataset):\n",
    "\n",
    "    #print test\n",
    "    myCount = 0\n",
    "    myClassLabels = []\n",
    "    ID = []\n",
    "    for feature in test_dataSet_totalSplit:\n",
    "        currentClassLabel = classify(myDecisionTree, featureNames, feature)\n",
    "        print myCount\n",
    "        print currentClassLabel\n",
    "    \n",
    "        myClassLabels.append(currentClassLabel)\n",
    "        ID.append(myCount)\n",
    "        myCount +=1\n",
    "        \n",
    "    return myClassLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "positive\n",
      "1\n",
      "negative\n",
      "2\n",
      "positive\n",
      "3\n",
      "positive\n",
      "4\n",
      "negative\n",
      "5\n",
      "negative\n",
      "6\n",
      "positive\n",
      "7\n",
      "positive\n",
      "8\n",
      "positive\n",
      "9\n",
      "negative\n",
      "10\n",
      "negative\n",
      "11\n",
      "positive\n",
      "12\n",
      "negative\n",
      "13\n",
      "negative\n",
      "14\n",
      "negative\n",
      "15\n",
      "positive\n",
      "16\n",
      "positive\n",
      "17\n",
      "negative\n",
      "18\n",
      "negative\n",
      "19\n",
      "positive\n",
      "20\n",
      "negative\n",
      "21\n",
      "positive\n",
      "22\n",
      "negative\n",
      "23\n",
      "positive\n",
      "24\n",
      "positive\n",
      "25\n",
      "positive\n",
      "26\n",
      "negative\n",
      "27\n",
      "[]\n",
      "28\n",
      "negative\n",
      "29\n",
      "positive\n",
      "30\n",
      "positive\n",
      "31\n",
      "positive\n",
      "32\n",
      "positive\n",
      "33\n",
      "negative\n",
      "34\n",
      "positive\n",
      "35\n",
      "positive\n",
      "36\n",
      "positive\n",
      "37\n",
      "positive\n",
      "38\n",
      "positive\n",
      "39\n",
      "negative\n",
      "40\n",
      "negative\n",
      "41\n",
      "positive\n",
      "42\n",
      "positive\n",
      "43\n",
      "negative\n",
      "44\n",
      "positive\n",
      "45\n",
      "positive\n",
      "46\n",
      "negative\n",
      "47\n",
      "negative\n",
      "48\n",
      "positive\n",
      "49\n",
      "positive\n",
      "50\n",
      "positive\n",
      "51\n",
      "negative\n",
      "52\n",
      "negative\n",
      "53\n",
      "positive\n",
      "54\n",
      "positive\n",
      "55\n",
      "positive\n",
      "56\n",
      "positive\n",
      "57\n",
      "positive\n",
      "58\n",
      "positive\n",
      "59\n",
      "positive\n",
      "60\n",
      "positive\n",
      "61\n",
      "positive\n",
      "62\n",
      "positive\n",
      "63\n",
      "positive\n",
      "64\n",
      "positive\n",
      "65\n",
      "positive\n",
      "66\n",
      "positive\n",
      "67\n",
      "positive\n",
      "68\n",
      "positive\n",
      "69\n",
      "[]\n",
      "70\n",
      "negative\n",
      "71\n",
      "negative\n",
      "72\n",
      "positive\n",
      "73\n",
      "negative\n",
      "74\n",
      "positive\n",
      "75\n",
      "positive\n",
      "76\n",
      "positive\n",
      "77\n",
      "positive\n",
      "78\n",
      "negative\n",
      "79\n",
      "positive\n",
      "80\n",
      "negative\n",
      "81\n",
      "positive\n",
      "82\n",
      "[]\n",
      "83\n",
      "negative\n",
      "84\n",
      "negative\n",
      "85\n",
      "positive\n",
      "86\n",
      "positive\n",
      "87\n",
      "positive\n",
      "88\n",
      "negative\n",
      "89\n",
      "negative\n",
      "90\n",
      "positive\n",
      "91\n",
      "[]\n",
      "92\n",
      "positive\n",
      "93\n",
      "positive\n",
      "94\n",
      "positive\n",
      "95\n",
      "positive\n",
      "96\n",
      "positive\n",
      "97\n",
      "positive\n",
      "98\n",
      "positive\n",
      "99\n",
      "positive\n",
      "100\n",
      "positive\n",
      "101\n",
      "positive\n",
      "102\n",
      "positive\n",
      "103\n",
      "[]\n",
      "104\n",
      "negative\n",
      "105\n",
      "positive\n",
      "106\n",
      "positive\n",
      "107\n",
      "positive\n",
      "108\n",
      "positive\n",
      "109\n",
      "positive\n",
      "110\n",
      "positive\n",
      "111\n",
      "negative\n",
      "112\n",
      "negative\n",
      "113\n",
      "positive\n",
      "114\n",
      "positive\n",
      "115\n",
      "positive\n",
      "116\n",
      "positive\n",
      "117\n",
      "negative\n",
      "118\n",
      "negative\n",
      "119\n",
      "[]\n",
      "120\n",
      "positive\n",
      "121\n",
      "positive\n",
      "122\n",
      "positive\n",
      "123\n",
      "positive\n",
      "124\n",
      "negative\n",
      "125\n",
      "positive\n",
      "126\n",
      "positive\n",
      "127\n",
      "negative\n",
      "128\n",
      "[]\n",
      "129\n",
      "positive\n",
      "130\n",
      "positive\n",
      "131\n",
      "negative\n",
      "132\n",
      "positive\n",
      "133\n",
      "positive\n",
      "134\n",
      "negative\n",
      "135\n",
      "[]\n",
      "136\n",
      "positive\n",
      "137\n",
      "positive\n",
      "138\n",
      "negative\n",
      "139\n",
      "positive\n",
      "140\n",
      "positive\n",
      "141\n",
      "positive\n",
      "142\n",
      "positive\n",
      "143\n",
      "positive\n",
      "144\n",
      "positive\n",
      "145\n",
      "positive\n",
      "146\n",
      "positive\n",
      "147\n",
      "positive\n",
      "148\n",
      "negative\n",
      "149\n",
      "negative\n",
      "150\n",
      "positive\n",
      "151\n",
      "negative\n",
      "152\n",
      "negative\n",
      "153\n",
      "positive\n",
      "154\n",
      "positive\n",
      "155\n",
      "positive\n",
      "156\n",
      "negative\n",
      "157\n",
      "positive\n",
      "158\n",
      "positive\n",
      "159\n",
      "negative\n",
      "160\n",
      "negative\n",
      "161\n",
      "positive\n",
      "162\n",
      "positive\n",
      "163\n",
      "positive\n",
      "164\n",
      "[]\n",
      "165\n",
      "negative\n",
      "166\n",
      "negative\n",
      "167\n",
      "positive\n",
      "168\n",
      "positive\n",
      "169\n",
      "negative\n",
      "170\n",
      "positive\n",
      "171\n",
      "[]\n",
      "172\n",
      "positive\n",
      "173\n",
      "negative\n",
      "174\n",
      "positive\n",
      "175\n",
      "positive\n",
      "176\n",
      "positive\n",
      "177\n",
      "positive\n",
      "178\n",
      "positive\n",
      "179\n",
      "positive\n",
      "180\n",
      "negative\n",
      "181\n",
      "positive\n",
      "182\n",
      "positive\n",
      "183\n",
      "negative\n",
      "184\n",
      "positive\n",
      "185\n",
      "negative\n",
      "186\n",
      "positive\n",
      "187\n",
      "positive\n",
      "188\n",
      "positive\n",
      "189\n",
      "positive\n",
      "190\n",
      "negative\n",
      "191\n",
      "negative\n",
      "192\n",
      "positive\n",
      "193\n",
      "positive\n",
      "194\n",
      "positive\n",
      "195\n",
      "positive\n",
      "196\n",
      "positive\n",
      "197\n",
      "positive\n",
      "198\n",
      "negative\n",
      "199\n",
      "[]\n",
      "200\n",
      "positive\n",
      "201\n",
      "negative\n",
      "202\n",
      "negative\n",
      "203\n",
      "negative\n",
      "204\n",
      "positive\n",
      "205\n",
      "positive\n",
      "206\n",
      "positive\n",
      "207\n",
      "positive\n",
      "208\n",
      "positive\n",
      "209\n",
      "positive\n",
      "210\n",
      "negative\n",
      "211\n",
      "positive\n",
      "212\n",
      "positive\n",
      "213\n",
      "positive\n",
      "214\n",
      "positive\n",
      "215\n",
      "negative\n",
      "216\n",
      "positive\n",
      "217\n",
      "positive\n",
      "218\n",
      "negative\n",
      "219\n",
      "positive\n",
      "220\n",
      "negative\n",
      "221\n",
      "positive\n",
      "222\n",
      "positive\n",
      "223\n",
      "positive\n",
      "224\n",
      "positive\n",
      "225\n",
      "positive\n",
      "226\n",
      "negative\n",
      "227\n",
      "positive\n",
      "228\n",
      "positive\n",
      "229\n",
      "positive\n",
      "230\n",
      "negative\n",
      "231\n",
      "positive\n",
      "232\n",
      "positive\n",
      "233\n",
      "positive\n",
      "234\n",
      "positive\n",
      "235\n",
      "positive\n",
      "236\n",
      "positive\n",
      "237\n",
      "negative\n",
      "238\n",
      "negative\n",
      "239\n",
      "positive\n",
      "240\n",
      "positive\n",
      "241\n",
      "positive\n",
      "242\n",
      "negative\n",
      "243\n",
      "positive\n",
      "244\n",
      "negative\n",
      "245\n",
      "negative\n",
      "246\n",
      "positive\n",
      "247\n",
      "positive\n",
      "248\n",
      "positive\n",
      "249\n",
      "positive\n",
      "250\n",
      "negative\n",
      "251\n",
      "positive\n",
      "252\n",
      "positive\n",
      "253\n",
      "positive\n",
      "254\n",
      "positive\n",
      "255\n",
      "positive\n",
      "256\n",
      "positive\n",
      "257\n",
      "positive\n",
      "258\n",
      "negative\n",
      "259\n",
      "negative\n",
      "260\n",
      "[]\n",
      "261\n",
      "negative\n",
      "262\n",
      "positive\n",
      "263\n",
      "positive\n",
      "264\n",
      "negative\n",
      "265\n",
      "positive\n",
      "266\n",
      "positive\n",
      "267\n",
      "positive\n",
      "268\n",
      "positive\n",
      "269\n",
      "negative\n",
      "270\n",
      "positive\n",
      "271\n",
      "positive\n",
      "272\n",
      "positive\n",
      "273\n",
      "positive\n",
      "274\n",
      "[]\n",
      "275\n",
      "[]\n",
      "276\n",
      "negative\n",
      "277\n",
      "positive\n",
      "278\n",
      "positive\n",
      "279\n",
      "positive\n",
      "280\n",
      "positive\n",
      "281\n",
      "negative\n",
      "282\n",
      "positive\n",
      "283\n",
      "positive\n",
      "284\n",
      "negative\n",
      "285\n",
      "positive\n",
      "286\n",
      "positive\n",
      "287\n",
      "negative\n",
      "288\n",
      "positive\n",
      "289\n",
      "positive\n",
      "290\n",
      "positive\n",
      "291\n",
      "positive\n",
      "292\n",
      "negative\n",
      "293\n",
      "positive\n",
      "294\n",
      "negative\n",
      "295\n",
      "negative\n",
      "296\n",
      "negative\n",
      "297\n",
      "negative\n",
      "298\n",
      "negative\n",
      "299\n",
      "positive\n",
      "300\n",
      "negative\n",
      "301\n",
      "[]\n",
      "302\n",
      "positive\n",
      "303\n",
      "negative\n",
      "304\n",
      "negative\n",
      "305\n",
      "positive\n",
      "306\n",
      "negative\n",
      "307\n",
      "negative\n",
      "308\n",
      "[]\n",
      "309\n",
      "positive\n",
      "310\n",
      "positive\n",
      "311\n",
      "negative\n",
      "312\n",
      "negative\n",
      "313\n",
      "negative\n",
      "314\n",
      "positive\n",
      "315\n",
      "positive\n",
      "316\n",
      "positive\n",
      "317\n",
      "negative\n",
      "318\n",
      "positive\n",
      "319\n",
      "positive\n",
      "320\n",
      "[]\n",
      "321\n",
      "positive\n",
      "322\n",
      "positive\n",
      "323\n",
      "positive\n",
      "324\n",
      "positive\n",
      "325\n",
      "negative\n",
      "326\n",
      "[]\n",
      "327\n",
      "[]\n",
      "328\n",
      "positive\n",
      "329\n",
      "positive\n",
      "330\n",
      "positive\n",
      "331\n",
      "negative\n",
      "332\n",
      "negative\n",
      "333\n",
      "positive\n",
      "334\n",
      "positive\n",
      "335\n",
      "negative\n",
      "336\n",
      "positive\n",
      "337\n",
      "negative\n",
      "338\n",
      "positive\n",
      "339\n",
      "negative\n",
      "340\n",
      "negative\n",
      "341\n",
      "negative\n",
      "342\n",
      "negative\n",
      "343\n",
      "positive\n",
      "344\n",
      "negative\n",
      "345\n",
      "negative\n",
      "346\n",
      "positive\n",
      "347\n",
      "negative\n",
      "348\n",
      "positive\n",
      "349\n",
      "positive\n",
      "350\n",
      "positive\n",
      "351\n",
      "positive\n",
      "352\n",
      "positive\n",
      "353\n",
      "negative\n",
      "354\n",
      "positive\n",
      "355\n",
      "positive\n",
      "356\n",
      "positive\n",
      "357\n",
      "positive\n",
      "358\n",
      "negative\n",
      "359\n",
      "negative\n",
      "360\n",
      "positive\n",
      "361\n",
      "positive\n",
      "362\n",
      "negative\n",
      "363\n",
      "negative\n",
      "364\n",
      "positive\n",
      "365\n",
      "positive\n",
      "366\n",
      "[]\n",
      "367\n",
      "positive\n",
      "368\n",
      "negative\n",
      "369\n",
      "positive\n",
      "370\n",
      "negative\n",
      "371\n",
      "negative\n",
      "372\n",
      "positive\n",
      "373\n",
      "positive\n",
      "374\n",
      "positive\n",
      "375\n",
      "[]\n",
      "376\n",
      "positive\n",
      "377\n",
      "negative\n",
      "378\n",
      "positive\n",
      "379\n",
      "positive\n",
      "380\n",
      "positive\n",
      "381\n",
      "positive\n",
      "382\n",
      "[]\n",
      "383\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "# TODO 5 Test:\n",
    "\n",
    "myPredictions = predict(myDecisionTree, featureNames_copy, test_dataSet_totalSplit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化（可选）\n",
    "在上面这些步骤完成后，你可以优化 create_decision_tree 函数以防止过拟合\n",
    "\n",
    "- 对决策树进行剪枝\n",
    "- 也推荐两个更简单又十分有效的办法\n",
    "    - 设置树的最大深度 max_depth\n",
    "    - 设置每个叶节点的最小 samples 数\n",
    "    - 这里可以参考 [decision tree in scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) 中的参数设置以及其原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mySubmit = pd.read_csv('my_submit.csv')\n",
    "\n",
    "# column_ID = pd.Series(ID, name='ID') \n",
    "# column_Category = pd.Series(classLabels, name = 'Category')\n",
    "# predictions = pd.concat([column_ID, column_Category], axis = 1)\n",
    "\n",
    "# predictions.to_csv('my_submit.csv', index = False)\n",
    "\n",
    "\n",
    "# 得到最终答案 y_pred, 是一个 1维的array\n",
    "# 存储为要求格式的文件\n",
    "\n",
    "df = pd.DataFrame(np.stack( (range(len(myPredictions)), myPredictions) ).T) \n",
    "df.to_csv('result.csv', index = None, header=['ID', 'Category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估\n",
    "\n",
    "下面的数据可在你的 predict 文件提交至 Kaggle 后获得。\n",
    "\n",
    "- Kaggle 昵称：\n",
    "- 模型目前 Public Leaderboard 得分：\n",
    "- 排名："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反思\n",
    "请对你的模型进行一定的分析，说出你模型的不足之处，或者可以提高的地方。\n",
    "\n",
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To improve the Model:\n",
    "import sklearn\n",
    "\n",
    "train_feature = pd.read_csv('train.csv')\n",
    "train_label = pd.read_csv('y_train.csv')\n",
    "\n",
    "train_feature_inArray = train_feature.values\n",
    "train_feature_inArray_noSerial = train_feature_inArray[:,1:]\n",
    "train_feature_totalSplit = train_feature_inArray_noSerial.tolist()\n",
    "\n",
    "train_label_inArray = train_label.values\n",
    "\n",
    "test_feature = pd.read_csv('test.csv')\n",
    "test_feature_inArray = test_feature.values\n",
    "test_feature_inArray_noSerial = test_feature_inArray[:,1:]\n",
    "test_feature_totalSplit = test_feature_inArray_noSerial.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-205-bdbcf92026a0>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-205-bdbcf92026a0>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    for j in range(len(train_feature_totalSplit[i]))\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_feature_totalSplit)):\n",
    "    for j in range(len(train_feature_totalSplit[i]))\n",
    "        if train_feature_totalSplit[i][j] == 'x':\n",
    "            train_feature_totalSplit[i][j] = 2\n",
    "        elif train_feature_totalSplit[i][j] == 'o':\n",
    "            train_feature_totalSplit[i][j] = 0\n",
    "        else:\n",
    "            train_feature_totalSplit[i][j] = 1\n",
    "            \n",
    "for i in range(len(test_feature_totalSplit)):\n",
    "    if test_feature_totalSplit[i] == 'x':\n",
    "        test_feature_totalSplit[i] = 2\n",
    "    elif i == 'o':\n",
    "        test_feature_totalSplit[i] = 0\n",
    "    else:\n",
    "        test_feature_totalSplit[i] = 1\n",
    "        \n",
    "train_feature_totalSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPredictModel = sklearn.neighbors.KNeighborsClassifier()\n",
    "myPredictModel.fit(train_feature, train_label)\n",
    "\n",
    "print myPredictionModel\n",
    "\n",
    "\n",
    "# 输出测试效果\n",
    "# print metrics.classification_report(expected, predicted)\n",
    "# print metrics.confusion_matrix(expected, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
