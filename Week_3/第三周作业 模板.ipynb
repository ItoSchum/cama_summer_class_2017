{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第三周作业 井字棋胜负判断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**摘要**：\n",
    "按照 Udacity ID3 Algorithm for Decision Trees (PDF) 中的介绍，用递归调用的方式，实现了一个基本的 ID3 决策树，用于根据井字棋的棋子布局对棋局胜负进行判断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题及数据集描述  \n",
    "### 问题：  \n",
    "    给出井字棋棋盘中所有空位的棋子布局，通过决策树实现对结果的预测  \n",
    "  \n",
    "### 数据集：  \n",
    "    train.csv: 训练集 棋子布局，即 train_feature  \n",
    "    y_train.csv: 训练集 比赛结果，即 train_label  \n",
    "    test.csv: 测试集 棋子布局，即 test_feature  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 算法原理\n",
    "\n",
    "### 算法 ID3\n",
    "\n",
    "讲述你的解决方案中以及探索过程中所用到算法及其原理。我们希望你在学习过程中就有记录笔记。可以整理后把对应放到此处。算法原理至少包含下述内容：\n",
    "\n",
    "- 输入和输出\n",
    "    - 输入：  \n",
    "        - 训练决策树：def create_decision_tree(dataSet, featureNames) \n",
    "            - dataSet 即训练集\n",
    "            - featureNames 即所有棋盘的空格\n",
    "            \n",
    "        - 预测结果：def predict(myDecisionTree, featureNames, test_dataset) \n",
    "            - test_dataset 即测试集棋子布局\n",
    "    - 输出：  \n",
    "        - 训练决策树：return myDecisionTree  \n",
    "        - 预测结果：return myClassLabels  \n",
    "\n",
    "\n",
    "- 原理（包含公式）  \n",
    "    1. 信息熵：\n",
    "    $Entropy(S) = \\sum_{i=1}^{c}\\log_2 p_i$\n",
    "    2. 信息增益：\n",
    "    $Gain(S, A) = Entropy(S) − \\sum_{v\\in Values(A)}\\frac{|Sv|}{|S|}Entropy(S_v)$\n",
    "\n",
    "\n",
    "- 算法所涉及的超参数所代表的含义、作用。尤其是在你的解决方案中涉及到的参数。对于这些超参数的值，你如何设定它们的值，改变这些值会对模型造成怎样的影响？\n",
    "    - 由于尚未设置最大深度，故没有超参数的设定\n",
    "    \n",
    "    \n",
    "- 算法的优点、缺点：\n",
    "    - 优点：逻辑较容易理解\n",
    "    - 缺点：容易在生成决策树的训练过程中出现过拟合的问题  \n",
    "\n",
    "\n",
    "- 对于算法中你还不理解的地方，也应该写下来。在下周的例会中，我们会尽量做出解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 解决方案\n",
    "- 正在尝试使用 skleran 中自带的 Class 如：LogisticRegression，KNeighborsClassifier，DecisionTreeClassifier 等进行训练，但需要将原 csv 文件中的 String 类型用 float 进行替换，尚未完成。\n",
    "\n",
    "详细的描述你对井字棋分类问题的具体解决方案。我们也建议你记录下整个探索过程，包括没有用在最终解决方案中的模型。模型的执行过程，以及过程中遇到的困难的描述应该清晰明了地记录和描述。\n",
    "\n",
    "要考虑下列问题：\n",
    "- 你所用到的算法和技术执行的方式是否清晰记录了？\n",
    "    是\n",
    "- 在运用上面所提及的技术及指标的执行过程中是否遇到了困难，是否需要作出改动来得到想要的结果？\n",
    "    是\n",
    "- 是否有需要重点解释的代码片段(例如复杂的函数）？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 实验结果\n",
    "\n",
    "实验的结果，以及分析。\n",
    "\n",
    "如果你尝试了多个模型，你可以用表格的形式呈现。\n",
    "\n",
    "下图是示例，请根据自己的情况修改。\n",
    "\n",
    "| 模型 | Local F1 score | Public Leaderboard |\n",
    "| --- | --- | --- |\n",
    "| ID3 决策树  |  | 0.80208 |\n",
    "| XGBoost |  |  |\n",
    "| Random Forest |  |  |\n",
    "| Ensemble |  | -  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "总结自己的工作，思考现有模型存在的问题，并指出可能的改进方法：   \n",
    "- 模型尚未设置树的最大深度 max_depth 和每个叶节点的最小 samples 数，在训练过程中容易出现过拟合的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码\n",
    "\n",
    "请把你的代码整理后放到下面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step_1: \n",
    "# Import package\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File train.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-414a70bac7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Read data from train.csv and y_train.csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/anaconda2-4.4.0/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/anaconda2-4.4.0/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/anaconda2-4.4.0/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/anaconda2-4.4.0/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/anaconda2-4.4.0/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File train.csv does not exist"
     ]
    }
   ],
   "source": [
    "# Step 2:\n",
    "# Read data from train.csv and y_train.csv\n",
    "\n",
    "train_feature = pd.read_csv('train.csv')\n",
    "train_label = pd.read_csv('y_train.csv')\n",
    "\n",
    "train_dataSet = pd.merge(train_feature, train_label, on = 'ID')\n",
    "\n",
    "featureNames = train_dataSet.columns.tolist()[1:10]\n",
    "featureNames_copy = train_dataSet.columns.tolist()[1:10]\n",
    "\n",
    "dataSet_inArray = train_dataSet.values\n",
    "dataSet_inArray_noSerial = dataSet_inArray[:,1:]\n",
    "dataSet_totalSplit = dataSet_inArray_noSerial.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3:\n",
    "# Calculate the entropy given a dataset\n",
    "\n",
    "def calculateShannonEntropy(dataSet): \n",
    "    numEntries = len(dataSet) # There are n rows inside\n",
    "    labelCounts = {} # Create dictionary for classification\n",
    "\n",
    "    for featureVector in dataSet:\n",
    "        \n",
    "    \tcurrentLabel = featureVector[-1] # Get the last-row data\n",
    "    \tif currentLabel not in labelCounts.keys():\n",
    "    \t\tlabelCounts[currentLabel] = 0\n",
    "    \tlabelCounts[currentLabel] += 1\n",
    "\n",
    "    total_entropy = 0.0\n",
    "    for key in labelCounts:\n",
    "    \tproportion_k = float(labelCounts[key]) / numEntries\n",
    "    \ttotal_entropy -= (proportion_k * math.log(proportion_k, 2))\n",
    "\n",
    "    return total_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3 Test:\n",
    "calculateShannonEntropy(dataSet_totalSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4:\n",
    "# Return the best feature based on the maximum number of information gain\n",
    "\n",
    "def choose_best_feature_to_split(dataSet):    \n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    baseEntropy = calculateShannonEntropy(dataSet)\n",
    "    bestInfoGain = 0\n",
    "    best_feature = -1\n",
    "\n",
    "    for i in range(numFeatures):\n",
    "    \tfeatureList = [number[i] for number in dataSet] # enum for one attribute\n",
    "    \tuniqualValues = set(featureList) # no-relace attribute\n",
    "    \tnewEntropy = 0\n",
    "\n",
    "    \tfor value in uniqualValues:\n",
    "    \t\tsub_dataset = split_dataset(dataSet, i, value)\n",
    "    \t\tproportion_k = len(sub_dataset) / float(len(dataSet))\n",
    "    \t\tnewEntropy += proportion_k * calculateShannonEntropy(sub_dataset) # sum(ShannonEntropy)\n",
    "    \tinfoGain = baseEntropy - newEntropy # infoGain\n",
    "\n",
    "    \t# bestInfoGain\n",
    "    \tif (infoGain > bestInfoGain):\n",
    "    \t\tbestInfoGain = infoGain\n",
    "    \t\tbest_feature = i\n",
    "\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4 Test:\n",
    "choose_best_feature_to_split(dataSet_totalSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 5: \n",
    "# Split the dataset via current selected feature and it's value\n",
    "# For example, when current_feature is TLS(top-left-square), and the value is 'o', \n",
    "# the task is that return the subdataset in which all \"TLS\" is equal to 'o'\n",
    "\n",
    "def split_dataset(dataSet, axis, value):\n",
    "    sub_dataset = []\n",
    "\n",
    "    for featureVector in dataSet:\n",
    "    \tif featureVector[axis] == value:\n",
    "    \t\treduceFeatureVector = featureVector[ :axis]\n",
    "    \t\treduceFeatureVector.extend(featureVector[axis+1: ])  \n",
    "    \t\tsub_dataset.append(reduceFeatureVector)\n",
    "\n",
    "    return sub_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 6: \n",
    "# Create a decision tree by recursion\n",
    "\n",
    "import operator\n",
    "def majorityCnt(classList):\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():classCount[vote] = 0\n",
    "        classCount[vote]+=1\n",
    "    sortedClassCount=sorted(classCount.items(),key = operator.itemgetter(1),reverse = True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "def create_decision_tree(dataSet, featureNames):    \n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    #类别相同，停止划分\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    #长度为1，返回出现次数最多的类别\n",
    "    if len(classList[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "\n",
    "    best_feature = choose_best_feature_to_split(dataSet) #返回分类的特征序号\n",
    "    bestFeatureName = featureNames[best_feature] #该特征的label\n",
    "    decision_tree = {bestFeatureName: { } }\n",
    "    del(featureNames[best_feature]) #从labels的list中删除该label\n",
    "    \n",
    "    featureValues = [example[best_feature] for example in dataSet]\n",
    "    uniqualValues = set(featureValues)\n",
    "    for value in uniqualValues:\n",
    "    \tsubFeatureNames = featureNames[ : ] #子集合\n",
    "\n",
    "    \t#构建数据的子集合，并进行递归\n",
    "    \tdecision_tree[bestFeatureName][value] = create_decision_tree(split_dataset(dataSet, best_feature, value), subFeatureNames)\n",
    "    \n",
    "    return decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 5&6 Test:\n",
    "myDecisionTree = create_decision_tree(dataSet_totalSplit, featureNames)\n",
    "myDecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 7\n",
    "# Func: classify\n",
    "\n",
    "def classify(inputTree, featureNames, testVector):\n",
    "    classLabel = []\n",
    "    firstStr = inputTree.keys()[0] #获取树的第一个特征属性\n",
    "    secondDict = inputTree[firstStr] #树的分支，子集合Dict\n",
    "    featureIndex = featureNames.index(firstStr) #获取决策树第一层在featLables中的位置\n",
    "    for key in secondDict.keys():\n",
    "        if testVector[featureIndex] == key:\n",
    "            if type(secondDict[key]).__name__ == 'dict':\n",
    "                classLabel = classify(secondDict[key], featureNames, testVector)\n",
    "            else:\n",
    "            \tclassLabel = secondDict[key]\n",
    "    \n",
    "    return classLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 7 Test\n",
    "# Func: classify Test:\n",
    "classLabel = classify(myDecisionTree, featureNames_copy, ['o', 'x', 'o', 'b', 'o', 'x', 'o', 'x', 'x'] )\n",
    "classLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 8\n",
    "# Get test_dataset from test.csv\n",
    "\n",
    "test_feature = pd.read_csv('test.csv')\n",
    "test_dataSet_totalSplit = test_feature.values[:,1:]\n",
    "\n",
    "def predict(myDecisionTree, featureNames, test_dataset):\n",
    "\n",
    "    #print test\n",
    "    myCount = 0\n",
    "    myClassLabels = []\n",
    "    ID = []\n",
    "    for feature in test_dataSet_totalSplit:\n",
    "        currentClassLabel = classify(myDecisionTree, featureNames, feature)\n",
    "        print myCount\n",
    "        print currentClassLabel\n",
    "    \n",
    "        myClassLabels.append(currentClassLabel)\n",
    "        ID.append(myCount)\n",
    "        myCount +=1\n",
    "        \n",
    "    return myClassLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 8 Test:\n",
    "\n",
    "myPredictions = predict(myDecisionTree, featureNames_copy, test_dataSet_totalSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 9:\n",
    "# Output\n",
    "# mySubmit = pd.read_csv('my_submit.csv')\n",
    "\n",
    "# 得到最终答案 y_pred, 是一个 1维的array\n",
    "# 存储为要求格式的文件\n",
    "\n",
    "df = pd.DataFrame(np.stack( (range(len(myPredictions)), myPredictions) ).T) \n",
    "df.to_csv('result.csv', index = None, header=['ID', 'Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
